{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url\n",
    "base_url = \"https://www.reuters.com\"\n",
    "page_endpoint = \"/news/archive/goldMktRpt?page=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform HTTP requests\n",
    "def send_http_request(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"HTTP Request Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape a page URL and return a list of article links\n",
    "def scrape_page(page_number):\n",
    "    page_url = f\"{base_url}{page_endpoint}{page_number}\"\n",
    "    page_html = send_http_request(page_url)\n",
    "    if page_html:\n",
    "        page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "        articles = page_soup.find_all('div', class_='story-content')\n",
    "        article_links = [f\"{base_url}{article.a.attrs['href']}\" for article in articles]\n",
    "        return article_links\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape an article link and return article information\n",
    "def scrape_article(article_link):\n",
    "    article_html = send_http_request(article_link)\n",
    "    if article_html:\n",
    "        article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "        article_info = {}\n",
    "        \n",
    "        article_json_meta_data = article_soup.find('script', type=\"application/ld+json\")\n",
    "        json_data = json.loads(article_json_meta_data.contents[0])\n",
    "        \n",
    "        utc_dt = article_info['datePublished'].replace(\"Z\", \"UTC\")\n",
    "        dt_obj = pd.to_datetime(utc_dt)\n",
    "        date = str(dt_obj.date())\n",
    "        time = str(dt_obj.time())\n",
    "        \n",
    "        article_info['date'] = date\n",
    "        article_info['time'] = time\n",
    "        article_info['link'] = article_link\n",
    "        article_info['headline'] = json_data['headline']\n",
    "        article_info['datePublished'] = json_data['datePublished']\n",
    "        article_info['author'] = json_data['author']['name']\n",
    "        article_info['type_of_author'] = json_data['author']['@type']\n",
    "        article_info['publisher'] = json_data['publisher']['name']\n",
    "        article_info['type_of_publisher'] = json_data['publisher']['@type']\n",
    "        \n",
    "        article_pre_tag = article_soup.find('pre')\n",
    "        if article_pre_tag:\n",
    "            article_text = article_pre_tag.text\n",
    "            article_text = article_text.replace('\\n', '').strip()\n",
    "        else:\n",
    "            article_body_wrapper = article_soup.find('div', class_='ArticleBodyWrapper')\n",
    "            article_text_paragraphs = article_body_wrapper.find_all('p', class_='Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x')\n",
    "            article_paragraphs = [paragraph.get_text(strip=True) for paragraph in article_text_paragraphs]\n",
    "            article_text = ' '.join(article_paragraphs).strip()\n",
    "            \n",
    "        article_info['type_of_publisher'] = article_text\n",
    "        return article_info\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the last scraped page number from a file\n",
    "def read_last_page_number(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            last_page = int(file.read().strip())\n",
    "        return last_page\n",
    "    except FileNotFoundError:\n",
    "        return 1  # Start from page 1 if the file doesn't exist\n",
    "\n",
    "# Function to write the last scraped page number to a file\n",
    "def write_last_page_number(filename, page_number):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(str(page_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape articles from a specified range of pages and save them to CSV\n",
    "def scrape_pages_and_save_csv(start_page, end_page):\n",
    "    article_info_list = []\n",
    "    last_scraped_page = read_last_page_number('last_scraped_page.txt')\n",
    "    \n",
    "    for page_number in range(start_page, last_scraped_page - 1, -1):\n",
    "        article_links = scrape_page(page_number)\n",
    "        if article_links:\n",
    "            for article_link in article_links:\n",
    "                article_info = scrape_article(article_link)\n",
    "                if article_info:\n",
    "                    article_info_list.append(article_info)\n",
    "                    if len(article_info_list) >= 10:\n",
    "                        save_to_csv(article_info_list, f'articles_page_{page_number}.csv')\n",
    "                        article_info_list = []\n",
    "                write_last_page_number('last_scraped_page.txt', page_number)\n",
    "        else:\n",
    "            print(f\"Failed to scrape page {page_number}. Skipping...\")\n",
    "\n",
    "    # Save any remaining articles\n",
    "    if article_info_list:\n",
    "        save_to_csv(article_info_list, f'articles_page_{last_scraped_page}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape articles from the last scraped page (or page 394) to page 1\n",
    "scrape_pages_and_save_csv(read_last_page_number('last_scraped_page.txt'), 1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
